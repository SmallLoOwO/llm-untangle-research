#!/usr/bin/env python3
"""
BCa (Bias-Corrected and Accelerated) Bootstrap ç½®ä¿¡å€é–“è¨ˆç®—
- è¼‰å…¥ Untangle åŸºç·šæ¸¬è©¦çµæœ
- é€²è¡Œ 10,000 æ¬¡ Bootstrap é‡æŠ½æ¨£
- è¨ˆç®— BCa ç½®ä¿¡å€é–“ï¼ˆé è¨­ 95%ï¼‰
- è¼¸å‡ºçµæœåˆ° results/bca_confidence_intervals.json
"""
import json
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from scipy import stats
import time

ROOT = Path(__file__).resolve().parents[1]
BASELINE_RESULTS_PATH = ROOT / 'results' / 'untangle_baseline_results.json'
RESULTS_DIR = ROOT / 'results'


def load_baseline_results():
    if not BASELINE_RESULTS_PATH.exists():
        raise FileNotFoundError(f'æ‰¾ä¸åˆ° {BASELINE_RESULTS_PATH}ï¼Œè«‹å…ˆåŸ·è¡Œ run_untangle_baseline.py')
    return json.loads(BASELINE_RESULTS_PATH.read_text(encoding='utf-8'))


def jackknife_bias_acceleration(data, statistic_func):
    """è¨ˆç®— Jackknife åå·®æ ¡æ­£èˆ‡åŠ é€Ÿåƒæ•¸"""
    n = len(data)
    theta_hat = statistic_func(data)
    
    # Jackknife çµæœ
    jackknife_estimates = []
    for i in range(n):
        # ç§»é™¤ç¬¬ i å€‹è§€æ¸¬å€¼
        jackknife_sample = np.concatenate([data[:i], data[i+1:]])
        jackknife_estimates.append(statistic_func(jackknife_sample))
    
    jackknife_estimates = np.array(jackknife_estimates)
    theta_dot = np.mean(jackknife_estimates)
    
    # åå·®æ ¡æ­£ z0
    proportion = np.sum(jackknife_estimates < theta_hat) / n
    z0 = stats.norm.ppf(proportion) if 0 < proportion < 1 else 0
    
    # åŠ é€Ÿåƒæ•¸ a
    numerator = np.sum((theta_dot - jackknife_estimates) ** 3)
    denominator = 6 * (np.sum((theta_dot - jackknife_estimates) ** 2) ** 1.5)
    a = numerator / denominator if denominator != 0 else 0
    
    return z0, a


def bca_bootstrap(data, statistic_func, n_bootstrap=10000, alpha=0.05):
    """
BCa (Bias-Corrected and Accelerated) Bootstrap ç½®ä¿¡å€é–“
åƒè€ƒï¼šEfron & Tibshirani (1993) An Introduction to the Bootstrap
"""
    n = len(data)
    
    # 1. Bootstrap é‡æŠ½æ¨£
    print(f'é€²è¡Œ {n_bootstrap:,} æ¬¡ Bootstrap é‡æŠ½æ¨£...')
    bootstrap_estimates = []
    
    np.random.seed(42)  # ç¢ºä¿çµæœå¯é‡ç¾
    for _ in tqdm(range(n_bootstrap), desc='Bootstrap æŠ½æ¨£'):
        bootstrap_sample = np.random.choice(data, size=n, replace=True)
        bootstrap_estimates.append(statistic_func(bootstrap_sample))
    
    bootstrap_estimates = np.array(bootstrap_estimates)
    
    # 2. è¨ˆç®— z0 èˆ‡ a
    print('è¨ˆç®— Bias-Correction èˆ‡ Acceleration åƒæ•¸...')
    z0, a = jackknife_bias_acceleration(data, statistic_func)
    
    # 3. è¨ˆç®—èª¿æ•´å¾Œçš„åˆ†ä½æ•¸
    z_alpha_2 = stats.norm.ppf(alpha / 2)
    z_1_alpha_2 = stats.norm.ppf(1 - alpha / 2)
    
    # BCa èª¿æ•´å…¬å¼
    alpha1 = stats.norm.cdf(z0 + (z0 + z_alpha_2) / (1 - a * (z0 + z_alpha_2)))
    alpha2 = stats.norm.cdf(z0 + (z0 + z_1_alpha_2) / (1 - a * (z0 + z_1_alpha_2)))
    
    # ç¢ºä¿åœ¨ [0, 1] ç¯„åœå…§
    alpha1 = max(0, min(1, alpha1))
    alpha2 = max(0, min(1, alpha2))
    
    # è¨ˆç®—ç½®ä¿¡å€é–“
    lower_bound = np.percentile(bootstrap_estimates, alpha1 * 100)
    upper_bound = np.percentile(bootstrap_estimates, alpha2 * 100)
    
    return {
        'lower_bound': float(lower_bound),
        'upper_bound': float(upper_bound),
        'bias_correction_z0': float(z0),
        'acceleration_a': float(a),
        'bootstrap_mean': float(np.mean(bootstrap_estimates)),
        'bootstrap_std': float(np.std(bootstrap_estimates)),
        'alpha1': float(alpha1),
        'alpha2': float(alpha2)
    }


def calculate_confidence_intervals():
    print('ğŸ“Š BCa Bootstrap ç½®ä¿¡å€é–“è¨ˆç®—')
    print('=' * 50)
    
    # è¼‰å…¥åŸºç·šæ¸¬è©¦çµæœ
    baseline_data = load_baseline_results()
    
    # æå–æº–ç¢ºç‡æ•¸æ“š
    test_df = pd.read_csv(ROOT / 'data' / 'processed' / 'test.csv')
    baseline_results = baseline_data.get('detailed_results', [])
    
    if len(baseline_results) < len(test_df):
        print('âš ï¸ åŸºç·šçµæœä¸å®Œæ•´ï¼Œé‡æ–°åŸ·è¡Œ run_untangle_baseline.py')
        return
    
    confidence_results = {}
    
    for layer in ['l1', 'l2', 'l3']:
        print(f'\nè¨ˆç®— {layer.upper()} å±¤ BCa ç½®ä¿¡å€é–“...')
        
        # æå–è©²å±¤çš„æº–ç¢ºç‡æ•¸æ“š
        layer_accuracy = [r['accuracy'][layer] for r in baseline_results]
        accuracy_array = np.array(layer_accuracy, dtype=float)
        
        # å®šç¾©çµ±è¨ˆé‡ï¼ˆå¹³å‡æº–ç¢ºç‡ï¼‰
        def accuracy_statistic(sample):
            return np.mean(sample)
        
        # è¨ˆç®— BCa ç½®ä¿¡å€é–“
        bca_result = bca_bootstrap(accuracy_array, accuracy_statistic, n_bootstrap=10000, alpha=0.05)
        
        confidence_results[layer] = bca_result
        
        print(f'{layer.upper()} æº–ç¢ºç‡: {bca_result["bootstrap_mean"]:.3f}')
        print(f'95% BCa ç½®ä¿¡å€é–“: [{bca_result["lower_bound"]:.3f}, {bca_result["upper_bound"]:.3f}]')
    
    # ä¿å­˜çµæœ
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    output = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'method': 'BCa Bootstrap',
        'n_bootstrap': 10000,
        'confidence_level': 0.95,
        'baseline_method': 'Untangle',
        'test_samples': len(baseline_results),
        'confidence_intervals': confidence_results
    }
    
    output_path = RESULTS_DIR / 'bca_confidence_intervals.json'
    output_path.write_text(json.dumps(output, indent=2, ensure_ascii=False), encoding='utf-8')
    
    print(f'\nâœ“ BCa ç½®ä¿¡å€é–“è¨ˆç®—å®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° {output_path}')
    return confidence_results


if __name__ == '__main__':
    calculate_confidence_intervals()