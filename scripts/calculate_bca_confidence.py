#!/usr/bin/env python3
"""
BCa (Bias-Corrected and Accelerated) Bootstrap ç½®ä¿¡å€é–“è¨ˆç®—
- åŸºæ–¼ Untangle åŸºç·šæ¸¬è©¦çµæœ
- è¨ˆç®—çµ±è¨ˆå­¸åš´è¬¹çš„ 95% ç½®ä¿¡å€é–“
- æ”¯æŒè«–æ–‡çš„çµ±è¨ˆé©—è­‰éœ€æ±‚
"""
import json
import numpy as np
from pathlib import Path
from tqdm import tqdm
from scipy import stats
import time

ROOT = Path(__file__).resolve().parents[1]
BASELINE_RESULTS_PATH = ROOT / 'results' / 'untangle_baseline_results.json'
RESULTS_DIR = ROOT / 'results'

def load_baseline_results():
    if not BASELINE_RESULTS_PATH.exists():
        raise FileNotFoundError(f'æ‰¾ä¸åˆ° {BASELINE_RESULTS_PATH}ï¼Œè«‹å…ˆåŸ·è¡Œ run_untangle_baseline.py')
    return json.loads(BASELINE_RESULTS_PATH.read_text(encoding='utf-8'))

def jackknife_bias_acceleration(data, statistic_func):
    """Jackknife åå·®æ ¡æ­£èˆ‡åŠ é€Ÿåƒæ•¸è¨ˆç®—"""
    n = len(data)
    if n < 5:
        return 0.0, 0.0
        
    theta_hat = statistic_func(data)
    
    # Jackknife ä¼°è¨ˆ
    jackknife_estimates = []
    for i in range(n):
        jackknife_sample = np.concatenate([data[:i], data[i+1:]])
        jackknife_estimates.append(statistic_func(jackknife_sample))
    
    jackknife_estimates = np.array(jackknife_estimates)
    theta_dot = np.mean(jackknife_estimates)
    
    # åå·®æ ¡æ­£ z0
    proportion = np.sum(jackknife_estimates < theta_hat) / n
    z0 = stats.norm.ppf(max(0.001, min(0.999, proportion)))
    
    # åŠ é€Ÿåƒæ•¸ a
    numerator = np.sum((theta_dot - jackknife_estimates) ** 3)
    denominator = 6 * (np.sum((theta_dot - jackknife_estimates) ** 2) ** 1.5)
    a = numerator / denominator if abs(denominator) > 1e-10 else 0
    
    return z0, a

def bca_bootstrap(data, statistic_func, n_bootstrap=10000, alpha=0.05):
    """BCa Bootstrap ç½®ä¿¡å€é–“è¨ˆç®—"""
    n = len(data)
    if n < 5:
        return {'error': f'æ¨£æœ¬æ•¸å¤ªå°‘ (n={n})ï¼Œç„¡æ³•é€²è¡Œ Bootstrap åˆ†æ'}
    
    print(f'é€²è¡Œ {n_bootstrap:,} æ¬¡ Bootstrap é‡æŠ½æ¨£...')
    bootstrap_estimates = []
    
    np.random.seed(42)  # ç¢ºä¿çµæœå¯é‡ç¾
    for _ in tqdm(range(n_bootstrap), desc='Bootstrap æŠ½æ¨£'):
        bootstrap_sample = np.random.choice(data, size=n, replace=True)
        bootstrap_estimates.append(statistic_func(bootstrap_sample))
    
    bootstrap_estimates = np.array(bootstrap_estimates)
    
    # è¨ˆç®— z0 èˆ‡ a
    print('è¨ˆç®— Bias-Correction èˆ‡ Acceleration åƒæ•¸...')
    z0, a = jackknife_bias_acceleration(data, statistic_func)
    
    # BCa èª¿æ•´
    z_alpha_2 = stats.norm.ppf(alpha / 2)
    z_1_alpha_2 = stats.norm.ppf(1 - alpha / 2)
    
    # é˜²æ­¢é™¤é›¶
    denom1 = 1 - a * (z0 + z_alpha_2)
    denom2 = 1 - a * (z0 + z_1_alpha_2)
    
    if abs(denom1) < 1e-10 or abs(denom2) < 1e-10:
        print('âš ï¸ BCa åƒæ•¸ç•°å¸¸ï¼Œä½¿ç”¨åŸºæœ¬ Bootstrap ç™¾åˆ†ä½æ•¸')
        alpha1, alpha2 = alpha/2, 1-alpha/2
    else:
        alpha1 = stats.norm.cdf(z0 + (z0 + z_alpha_2) / denom1)
        alpha2 = stats.norm.cdf(z0 + (z0 + z_1_alpha_2) / denom2)
        alpha1 = max(0.001, min(0.999, alpha1))
        alpha2 = max(0.001, min(0.999, alpha2))
    
    lower_bound = np.percentile(bootstrap_estimates, alpha1 * 100)
    upper_bound = np.percentile(bootstrap_estimates, alpha2 * 100)
    
    return {
        'lower_bound': float(lower_bound),
        'upper_bound': float(upper_bound),
        'bias_correction_z0': float(z0),
        'acceleration_a': float(a),
        'bootstrap_mean': float(np.mean(bootstrap_estimates)),
        'bootstrap_std': float(np.std(bootstrap_estimates)),
        'alpha1': float(alpha1),
        'alpha2': float(alpha2),
        'original_estimate': float(statistic_func(data)),
        'bootstrap_samples': n_bootstrap
    }

def calculate_confidence_intervals():
    print('ğŸ“Š BCa Bootstrap ç½®ä¿¡å€é–“è¨ˆç®—ï¼ˆè«–æ–‡çµ±è¨ˆé©—è­‰ï¼‰')
    print('=' * 60)
    
    baseline_data = load_baseline_results()
    detailed_results = baseline_data.get('detailed_results', [])
    
    if not detailed_results:
        print('âŒ åŸºç·šçµæœä¸­ç„¡è©³ç´°æ•¸æ“šï¼Œè«‹é‡æ–°åŸ·è¡Œ run_untangle_baseline.py')
        return None
    
    print(f'è¼‰å…¥ {len(detailed_results)} å€‹åŸºç·šæ¸¬è©¦çµæœ')
    
    # æå– L3 æº–ç¢ºç‡æ•¸æ“š
    l3_accuracy = [r['accuracy']['l3'] for r in detailed_results]
    accuracy_array = np.array(l3_accuracy, dtype=float)
    
    if len(accuracy_array) < 5:
        print(f'âš ï¸ L3 æ¨£æœ¬æ•¸ä¸è¶³ ({len(accuracy_array)})ï¼Œè·³éçµ±è¨ˆåˆ†æ')
        return None
    
    print(f'\nè¨ˆç®— L3 æº–ç¢ºç‡ BCa ç½®ä¿¡å€é–“...')
    print(f'æ¨£æœ¬æ•¸: {len(accuracy_array)}')
    print(f'åŸå§‹æº–ç¢ºç‡: {np.mean(accuracy_array):.3f}')
    
    # å®šç¾©çµ±è¨ˆé‡ï¼ˆå¹³å‡æº–ç¢ºç‡ï¼‰
    def accuracy_statistic(sample):
        return np.mean(sample)
    
    # è¨ˆç®— BCa ç½®ä¿¡å€é–“
    bca_result = bca_bootstrap(accuracy_array, accuracy_statistic, n_bootstrap=10000, alpha=0.05)
    
    if 'error' in bca_result:
        print(f'âŒ L3 BCa è¨ˆç®—å¤±æ•—: {bca_result["error"]}')
        return None
    
    print(f'\nL3 æº–ç¢ºç‡çµ±è¨ˆçµæœ:')
    print(f'åŸå§‹ä¼°è¨ˆ: {bca_result["original_estimate"]:.3f}')
    print(f'Bootstrap å¹³å‡: {bca_result["bootstrap_mean"]:.3f} Â± {bca_result["bootstrap_std"]:.3f}')
    print(f'95% BCa ç½®ä¿¡å€é–“: [{bca_result["lower_bound"]:.3f}, {bca_result["upper_bound"]:.3f}]')
    print(f'åå·®æ ¡æ­£ z0: {bca_result["bias_correction_z0"]:.4f}')
    print(f'åŠ é€Ÿåƒæ•¸ a: {bca_result["acceleration_a"]:.4f}')
    
    # ä¿å­˜å®Œæ•´çµ±è¨ˆçµæœ
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    output = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'method': 'BCa Bootstrap',
        'confidence_level': 0.95,
        'baseline_method': 'Untangle Enhanced (OOD)',
        'test_samples': len(detailed_results),
        'l3_accuracy_analysis': bca_result,
        'statistical_summary': {
            'sample_size': len(accuracy_array),
            'success_count': int(np.sum(accuracy_array)),
            'failure_count': int(len(accuracy_array) - np.sum(accuracy_array)),
            'success_rate': float(np.mean(accuracy_array)),
            'confidence_interval_width': float(bca_result['upper_bound'] - bca_result['lower_bound'])
        }
    }
    
    output_path = RESULTS_DIR / 'bca_confidence_intervals.json'
    output_path.write_text(json.dumps(output, indent=2, ensure_ascii=False), encoding='utf-8')
    
    print(f'\nâœ… BCa ç½®ä¿¡å€é–“è¨ˆç®—å®Œæˆï¼Œçµæœå·²ä¿å­˜åˆ° {output_path}')
    
    # çµ±è¨ˆè§£é‡‹
    ci_width = bca_result['upper_bound'] - bca_result['lower_bound']
    print(f'\nğŸ“ˆ çµ±è¨ˆè§£é‡‹:')
    print(f'ç½®ä¿¡å€é–“å¯¬åº¦: {ci_width:.3f} ({"çª„" if ci_width < 0.2 else "ä¸­ç­‰" if ci_width < 0.4 else "å¯¬"})')
    print(f'çµ±è¨ˆé¡¯è‘—æ€§: {"æ˜¯" if bca_result["lower_bound"] > 0.5 else "å¦"} (ä¸‹ç•Œ > 0.5)')
    print(f'è«–æ–‡çµè«–æ”¯æŒ: 95% ç½®ä¿¡å€é–“ç‚º [{bca_result["lower_bound"]:.3f}, {bca_result["upper_bound"]:.3f}]')
    
    return bca_result

if __name__ == '__main__':
    try:
        result = calculate_confidence_intervals()
        exit(0 if result else 1)
    except Exception as e:
        print(f'âŒ BCa è¨ˆç®—å¤±æ•—: {e}')
        exit(1)